# Architecture Overview

This project is a Next.js application that allows employees to log order status lines and chat with an AI assistant. The stack uses a SQLite database, server-side API routes, and Google Gemini for LLM capabilities.

## Components

### Database
- **SQLite** is used as the persistence layer (`database.sqlite`).
- Tables:
  - `users` – stores user credentials (`id`, `username`, `password`).
  - `lines` – each row of a daily log (`user_id`, `date`, `idx`, `content`, `last_modified`).
  - `smart_tuples` – cached summary text per work order ID (`id`, `summary`, `last_updated`).
  - Database helpers in `lib/db.ts` provide CRUD operations. A new `getAllNotes` function aggregates all lines per user and date and reports each note's last modified timestamp.

### Authentication
- Sessions are signed tokens stored in a cookie. Creation and verification logic lives in `lib/auth.ts`.
- API routes verify the session before serving data.

### API Routes
- `/api/login`, `/api/register`, `/api/logout` – manage auth.
- `/api/notes` – fetch or update a user's daily lines.
- **`/api/chat`** – new route that collects all lines and sends them to Gemini 2.5 Flash for a single-turn response.
- `/api/smarttuple` – returns a cached per-order summary generated by Gemini. The journal UI calls this endpoint a few seconds after edits stop so summaries stay up to date.

### LLM Integration
1. `app/api/chat/route.ts` initializes `GoogleGenAI` with an API key from `GOOGLE_API_KEY`.
2. `app/api/smarttuple/route.ts` exposes a GET endpoint that refreshes a JSON summary for each work order ID when the underlying notes change. The journal calls this after edits settle. Summaries are stored in the `smart_tuples` table.
3. When a POST request with `{ question }` arrives to `/api/chat`:
   - Session is verified.
   - All lines are loaded with `getAllNotes()`, which also provides the last modified timestamp for each note.
   - Lines along with their last modified time are concatenated and sent to the `gemini-2.5-flash` model together with the user's question.
   - The API returns the model's text reply.

### Client Application
- `JournalApp.tsx` provides the UI for daily status lines and a management chat interface.
- The management view posts questions to `/api/chat` and displays the response.
- Daily lines are saved automatically and retrieved via `/api/notes`.

## Data Flow
1. **User Authenticates** – credentials are validated; a session cookie is issued.
2. **User Enters Lines** – lines are saved to SQLite via `/api/notes`.
3. **User Queries AI** – a POST to `/api/chat` with the question. The server gathers all lines and queries Gemini.
4. **Smart Tuple Summary** – a few seconds after editing stops the journal calls `/api/smarttuple`. The server regenerates summaries if any notes changed and returns the cached results.
5. **LLM Response** – the answer is returned and shown in the management chat view.

This architecture keeps the database and AI interaction on the server, while the UI remains a thin Next.js client. To scale, the SQLite database could be replaced with a networked database and the API routes deployed on a serverless or Node environment with access to Google Gemini.
